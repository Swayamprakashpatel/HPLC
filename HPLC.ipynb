{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swayamprakashpatel/HPLC/blob/main/HPLC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "hdOKQlX8nYHN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Dropout\n",
        "import pandas_datareader as web\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from google.colab import files\n",
        "import time as tm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Data.csv')\n",
        "df = pd.DataFrame(df)\n",
        "df = df.iloc[:,:]\n",
        "\n",
        "X1 = df.iloc[:, 7:11]\n",
        "X1 = X1.div(100).round(2)\n",
        "\n",
        "X2 = df.iloc[:,11:12]\n",
        "X2 = X2.div(14).round(2)\n",
        "\n",
        "X3 = df.iloc[:, 12:1774]\n",
        "\n",
        "X = [X1,X2,X3]\n",
        "X = pd.concat(X, axis=1)\n",
        "\n",
        "Y_Cat = df.iloc[:, 4:7]\n",
        "\n",
        "Y_Num = df.iloc[:,1774:1776]\n",
        "\n",
        "\n",
        "Y_Num = Y_Num.div(50).round(2)\n",
        "\n"
      ],
      "metadata": {
        "id": "pWFUSWyuSmAe"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_Num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CrzoLVj7t-r",
        "outputId": "0d2f181c-29ce-4cbc-859e-56e1d0600440"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     RT_Drug_1  RT_Drug_2\n",
            "0         0.10       0.18\n",
            "1         0.08       0.10\n",
            "2         0.08       0.12\n",
            "3         0.06       0.10\n",
            "4         0.06       0.04\n",
            "..         ...        ...\n",
            "963       0.06       0.10\n",
            "964       0.10       0.04\n",
            "965       0.10       0.04\n",
            "966       0.10       0.02\n",
            "967       0.08       0.18\n",
            "\n",
            "[968 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl6TPQWc79WI"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "id": "BZw3_4D4DTRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "REGRESSION"
      ],
      "metadata": {
        "id": "pQSckBk2XQy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y = Y_Num\n",
        "\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X, Y, test_size=0.3,random_state = 42 )\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5, random_state= 42)\n",
        "import numpy as np\n",
        " \n",
        "X_train = np.asarray(X_train).astype(np.int64)\n",
        "X_val = np.asarray(X_val).astype(np.int64)\n",
        "X_test = np.asarray(X_test).astype(np.int64)\n",
        "Y_train = np.asarray(Y_train).astype(np.int64)\n",
        "Y_val = np.asarray(Y_val).astype(np.int64)\n",
        "Y_test = np.asarray(Y_test).astype(np.int64)\n",
        " \n",
        "filepath = '/content/drive/My Drive/HPLC_NUM.hdf5'\n",
        " \n",
        "checkpoint = [tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_root_mean_squared_error', mode='min', save_best_only=True, Save_weights_only = False, verbose = 2), \n",
        "              tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=75, verbose =2)]\n",
        "output_nodes = Y.shape[1]\n",
        "print(output_nodes)\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Dense(512, activation='relu', input_shape=(1767,)),\n",
        "                             tf.keras.layers.Dense(512, activation='relu'),\n",
        "                             #tf.keras.layers.Dense(512, activation='relu'),\n",
        "                             tf.keras.layers.Dense(output_nodes, activation ='relu')])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate = 0.7), loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "hist = model.fit(X, Y, epochs= 10000, callbacks=[checkpoint],validation_data=(X, Y), batch_size= 100)\n",
        "model.evaluate(X_test, Y_test)\n",
        " \n",
        "# Error Graph\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(hist.history['root_mean_squared_error'])\n",
        "plt.plot(hist.history['val_root_mean_squared_error'])\n",
        "plt.title('Model RMSE')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "plt.show()\n",
        " \n",
        "\n",
        "train_acc = min(hist.history['root_mean_squared_error'])\n",
        "val_acc = min(hist.history['val_root_mean_squared_error'])\n",
        "train_loss = min(hist.history['loss'])\n",
        "val_loss = min(hist.history['val_loss'])\n",
        "print('Training RMSE is')\n",
        "print(train_acc)\n",
        "print('Validation RMSE is')\n",
        "print(val_acc)\n",
        "print('Training loss is')\n",
        "print(train_loss)\n",
        "print('Validation loss is')\n",
        "print(val_loss)\n",
        "\n"
      ],
      "metadata": {
        "id": "_VAAX6AmXSVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "220b1f14-3c7a-43b7-dea5-66d9a8dadd15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "Epoch 1/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 275644810264576.0000 - root_mean_squared_error: 16602554.0000\n",
            "Epoch 1: val_root_mean_squared_error improved from inf to 0.12289, saving model to /content/drive/My Drive/HPLC_NUM.hdf5\n",
            "10/10 [==============================] - 1s 47ms/step - loss: 275644810264576.0000 - root_mean_squared_error: 16602554.0000 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 2/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1227\n",
            "Epoch 2: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 31ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 3/10000\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 0.0152 - root_mean_squared_error: 0.1233\n",
            "Epoch 3: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 34ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 4/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 4: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 31ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 5/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 5: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 31ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 6/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 6: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 33ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 7/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 7: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 33ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 8/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0153 - root_mean_squared_error: 0.1237\n",
            "Epoch 8: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 31ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 9/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1230\n",
            "Epoch 9: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 33ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 10/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 10: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 30ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 11/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 11: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 29ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 12/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 12: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 32ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 13/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 13: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 31ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 14/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 14: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 33ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 15/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 15: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 31ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 16/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 16: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 32ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 17/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 17: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 31ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 18/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 18: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 30ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 19/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1230\n",
            "Epoch 19: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 32ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 20/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 20: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 33ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 21/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 21: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 31ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 22/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 22: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 34ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 23/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 23: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 30ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 24/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 24: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 32ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 25/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1228\n",
            "Epoch 25: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 33ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 26/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0154 - root_mean_squared_error: 0.1240\n",
            "Epoch 26: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 33ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 27/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 27: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 31ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 28/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 28: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 32ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 29/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1230\n",
            "Epoch 29: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 31ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 30/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 30: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 32ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 31/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 31: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 33ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 32/10000\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 0.0150 - root_mean_squared_error: 0.1224\n",
            "Epoch 32: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 33ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 33/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 33: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 33ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 34/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 34: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 30ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 35/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0152 - root_mean_squared_error: 0.1234\n",
            "Epoch 35: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 32ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 36/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0152 - root_mean_squared_error: 0.1233\n",
            "Epoch 36: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 31ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 37/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 37: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 29ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 38/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1230\n",
            "Epoch 38: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 33ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 39/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 39: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 32ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 40/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 40: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 33ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 41/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0148 - root_mean_squared_error: 0.1218\n",
            "Epoch 41: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 35ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 42/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 42: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 34ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 43/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 43: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 32ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 44/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 44: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 33ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 45/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 45: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 33ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 46/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 46: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 29ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 47/10000\n",
            " 8/10 [=======================>......] - ETA: 0s - loss: 0.0147 - root_mean_squared_error: 0.1213\n",
            "Epoch 47: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 33ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 48/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0147 - root_mean_squared_error: 0.1213\n",
            "Epoch 48: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 33ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 49/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 49: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 31ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 50/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 50: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 30ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 51/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0153 - root_mean_squared_error: 0.1238\n",
            "Epoch 51: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 32ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 52/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 52: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 31ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 53/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 53: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 30ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 54/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 54: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 32ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 55/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0153 - root_mean_squared_error: 0.1238\n",
            "Epoch 55: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 32ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 56/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0148 - root_mean_squared_error: 0.1215\n",
            "Epoch 56: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 31ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 57/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 57: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 32ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 58/10000\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1229\n",
            "Epoch 58: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 34ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 59/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1230\n",
            "Epoch 59: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 0s 36ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 60/10000\n",
            " 9/10 [==========================>...] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1228\n",
            "Epoch 60: val_root_mean_squared_error did not improve from 0.12289\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.0151 - root_mean_squared_error: 0.1229 - val_loss: 0.0151 - val_root_mean_squared_error: 0.1229\n",
            "Epoch 61/10000\n",
            " 6/10 [=================>............] - ETA: 0s - loss: 0.0151 - root_mean_squared_error: 0.1228"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Error Graph\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(hist.history['root_mean_squared_error'])\n",
        "plt.plot(hist.history['val_root_mean_squared_error'])\n",
        "plt.title('Model RMSE')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "plt.show()\n",
        " \n",
        "\n",
        "train_acc = min(hist.history['root_mean_squared_error'])\n",
        "val_acc = min(hist.history['val_root_mean_squared_error'])\n",
        "train_loss = min(hist.history['loss'])\n",
        "val_loss = min(hist.history['val_loss'])\n",
        "print('Training RMSE is')\n",
        "print(train_acc)\n",
        "print('Validation RMSE is')\n",
        "print(val_acc)\n",
        "print('Training loss is')\n",
        "print(train_loss)\n",
        "print('Validation loss is')\n",
        "print(val_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "X0vvwMORct5O",
        "outputId": "0320ea10-832a-4871-e7fd-5cdd727be927"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAdrElEQVR4nO3dfZRddX3v8ffnPCTBBAIJQSQTmCAIBMGETgMorQlcLagleA2a1FVD4ZbKlaJSSwNXKbjsvaULgVLpbWlBEFsCF8XmKhhFQK0PwPAoScg1QpBBhCSEhKckM5Pv/WP/zsyeMycwE2bPGWY+r7XOyjl773PmOzMn5zO/h/3bigjMzMzqlZpdgJmZjUwOCDMza8gBYWZmDTkgzMysIQeEmZk15IAwM7OGHBBmAySpVVJIqgzg2NMk/edw1GVWFAeEjUqS1knaLmnvuu0Ppg/51uZU1idoXkq3dZKW1h0zoPoltUj6hqQNkjZLelTSaTv5OrXbx4bpW7U3OQeEjWZPAItrDyQdAbyleeX0s2dETAIWAl+Q9L66/QOp/wbgKeAAYCrwx8Czjb5O7nbTUH4TNno5IGw0uwH4RO7xEuBr+QMkTZb0NUnrJT0p6fOSSmlfWdKl6a/zx4EPNnjuNZKekfS0pC9JKg+2yIhoB1YCswdbP/C7wHUR8XJEdEXEgxFx+2BrMGvEAWGj2c+BPSQdlj64FwFfrzvmH4DJwIHAe8k+kP8k7ftT4EPAHKCN7C/9vOuALuCgdMz7gf822CIlHQO8E1i7C/X/HLhK0iJJ+w/2a5u9FgeEjXa1v8LfB6wGnq7tyH3onh8RL0bEOuDLZN00AB8FroiIpyLieeB/5Z77VuADwGfSX+/PAZen1xuoDZJeBX4G/CPwrcHUn5wK/Bj4AvCEpIck/W6Dr/NC7nbYIGq0Mex1Z2OYvcndAPwImEn/7pm9gSrwZG7bk8D0dH8/sv79/L6aA9Jzn5FU21aqO/717A0E8Gngj9LrbR9E/UTEJmApsDQNaF8KfEtSS/7rRETXIOoyA9yCsFEuIp4kG+z9APDNut0bgE6yD/ua/en9K/0ZYEbdvpqngG1kH757ptseEXH4IOvrjojLgK3Afx9k/fXHbiALiP2AKYOpw6wRB4SNBWcAx0fEy/mNEdEN3Az8jaTdJR0AnEtvP//NwDlpKuleZH+p1577DPA94MuS9pBUkvR2Se/dxRr/FjhP0oSB1g8g6RJJ75RUkbQ7cBawNiI27mIdZj0cEDbqRcSv0kyhRv4ceBl4HPhP4N+Ba9O+fwFWAA8DD9D/L/hPAOOAVcAm4BbgbbtY5nfSa/zpIOt/C3Ar8EL6Hg4ATq475oW68yDO3cUabYyRLxhkZmaNuAVhZmYNOSDMzKwhB4SZmTXkgDAzs4ZGzYlye++9d7S2tja7DDOzN5X7779/Q0RMa7Rv1AREa2sr7e07mwloZmaNSHpyZ/vcxWRmZg05IMzMrCEHhJmZNTRqxiDMzAars7OTjo4Otm7d2uxSCjdhwgRaWlqoVqsDfo4DwszGrI6ODnbffXdaW1vJLds+6kQEGzdupKOjg5kzZw74ee5iMrMxa+vWrUydOnVUhwOAJKZOnTrolpIDwszGtNEeDjW78n2O+YB4ZvOrXPa9NTy+/qVml2JmNqKM+YB4bss2rrxzLU9s6HctFjOzQm3cuJHZs2cze/Zs9t13X6ZPn97zePv2+qvP9tXe3s4555xTaH1jfpC6Us6aXZ3dvi6GmQ2vqVOn8tBDDwFw0UUXMWnSJD73uc/17O/q6qJSafwx3dbWRltbW6H1jfkWRLWc/Qi6duxociVmZnDaaafxyU9+kqOPPprzzjuPe++9l2OPPZY5c+bw7ne/mzVr1gBw991386EPfQjIwuX0009n3rx5HHjggVx55ZVDUotbEKWsBdHlFoTZmHbx/13Jqt9sGdLXnLXfHvz1Hx4+6Od1dHTw05/+lHK5zJYtW/jxj39MpVLhjjvu4IILLuAb3/hGv+c89thj3HXXXbz44osccsghnHXWWYM656GRMR8QtRZEZ7dbEGY2Mpx66qmUy2UANm/ezJIlS/jlL3+JJDo7Oxs+54Mf/CDjx49n/Pjx7LPPPjz77LO0tLS8oTrGfEDUxiC6drgFYTaW7cpf+kWZOHFiz/0vfOELzJ8/n1tvvZV169Yxb968hs8ZP358z/1yuUxXV9cbrmPMj0FUSmkMwi0IMxuBNm/ezPTp0wG47rrrhvVrj/mAqHoWk5mNYOeddx7nn38+c+bMGZJWwWAoYnR8MLa1tcWuXDDopW1dvPOvV3DBBw7lzN9/ewGVmdlItXr1ag477LBmlzFsGn2/ku6PiIbzZQttQUg6UdIaSWslLW2wf7ykm9L+eyS1pu1VSddL+oWk1ZLOL6rG2iwmtyDMzPoqLCAklYGrgJOAWcBiSbPqDjsD2BQRBwGXA5ek7acC4yPiCOB3gD+rhcdQ6zkPwgFhZtZHkS2IucDaiHg8IrYDy4AFdccsAK5P928BTlC2olQAEyVVgN2A7cDQTlBOUgPCJ8qZmdUpMiCmA0/lHnekbQ2PiYguYDMwlSwsXgaeAX4NXBoRz9d/AUlnSmqX1L5+/fpdKlIS1bLcxWRmVmekzmKaC3QD+wEzgb+QdGD9QRFxdUS0RUTbtGnTdvmLVUolut2CMDPro8iAeBqYkXvckrY1PCZ1J00GNgJ/BHw3Ijoj4jngJ0Bhq1JV3IIwM+unyIC4DzhY0kxJ44BFwPK6Y5YDS9L9hcCdkc27/TVwPICkicAxwGNFFVotlzwGYWbDbv78+axYsaLPtiuuuIKzzjqr4fHz5s1jV6bz76rCAiKNKZwNrABWAzdHxEpJX5R0cjrsGmCqpLXAuUBtKuxVwCRJK8mC5qsR8UhRtVZK8iwmMxt2ixcvZtmyZX22LVu2jMWLFzepor4KXYspIm4DbqvbdmHu/layKa31z3up0faiVMsldzGZ2bBbuHAhn//859m+fTvjxo1j3bp1/OY3v+HGG2/k3HPP5dVXX2XhwoVcfPHFTalvzC/WB9kYhLuYzMa425fCb38xtK+57xFw0t/udPeUKVOYO3cut99+OwsWLGDZsmV89KMf5YILLmDKlCl0d3dzwgkn8Mgjj3DkkUcObW0DMFJnMQ0rdzGZWbPku5lq3Us333wzRx11FHPmzGHlypWsWrWqKbW5BUGti8ktCLMx7TX+0i/SggUL+OxnP8sDDzzAK6+8wpQpU7j00ku577772GuvvTjttNPYunVrU2pzC4JaF5NbEGY2/CZNmsT8+fM5/fTTWbx4MVu2bGHixIlMnjyZZ599lttvv71ptbkFQXainFsQZtYsixcv5sMf/jDLli3j0EMPZc6cORx66KHMmDGD97znPU2rywFBdk0Ij0GYWbOccsop5C+9sLMLA919993DU1DiLiayFoRnMZmZ9eWAwEttmJk14oDAS22YjWWj5aqar2dXvk8HBD4PwmysmjBhAhs3bhz1IRERbNy4kQkTJgzqeR6kxudBmI1VLS0tdHR0sKvXk3kzmTBhAi0tLYN6jgMCnwdhNlZVq1VmzpzZ7DJGLHcxkWYxuYvJzKwPBwSkS466i8nMLM8BgbuYzMwacUDgpTbMzBpxQOClNszMGnFAAGUvtWFm1o8DgtogdYz6k2XMzAbDAUE2BgHgcWozs14OCLJZTIAHqs3MchwQZF1MgKe6mpnlOCDo7WLqcgvCzKyHA4LeFoSvCWFm1ssBAVTKqQXhqa5mZj0cEGTXgwB8spyZWY4Dgux6EOBZTGZmeQ4Ieqe5ehaTmVkvBwS9s5jcgjAz6+WAIHcehMcgzMx6OCDwLCYzs0YcEEC15PMgzMzqOSDItSAcEGZmPRwQ5BbrcxeTmVkPBwRQLbkFYWZWzwFB7jwIT3M1M+vhgCC3WJ9PlDMz61FoQEg6UdIaSWslLW2wf7ykm9L+eyS15vYdKelnklZK+oWkCUXV6eW+zcz6KywgJJWBq4CTgFnAYkmz6g47A9gUEQcBlwOXpOdWgK8Dn4yIw4F5QGdRtVZ8opyZWT9FtiDmAmsj4vGI2A4sAxbUHbMAuD7dvwU4QZKA9wOPRMTDABGxMSK6iyq0Z7E+z2IyM+tRZEBMB57KPe5I2xoeExFdwGZgKvAOICStkPSApPMafQFJZ0pql9S+fv36XS607OW+zcz6GamD1BXgOODj6d8PSzqh/qCIuDoi2iKibdq0abv8xaperM/MrJ8iA+JpYEbucUva1vCYNO4wGdhI1tr4UURsiIhXgNuAo4oq1Mt9m5n1V2RA3AccLGmmpHHAImB53THLgSXp/kLgzogIYAVwhKS3pOB4L7CqqEJrAdHtgDAz61Ep6oUjokvS2WQf9mXg2ohYKemLQHtELAeuAW6QtBZ4nixEiIhNki4jC5kAbouI7xRVq7uYzMz6KywgACLiNrLuofy2C3P3twKn7uS5Xyeb6lq4UkmU5EFqM7O8kTpIPewq5ZKnuZqZ5TggkmpJbkGYmeU4IJJKueSlNszMchwQSbUsL9ZnZpbjgEgqJbcgzMzyHBBJpewxCDOzPAdEUi2X3MVkZpbjgEgqJbmLycwsxwGRVMolOt3FZGbWwwGRVMuiyyfKmZn1cEAkFZ8oZ2bWhwMiybqY3IIwM6txQCRZF5NbEGZmNQ6IxCfKmZn15YBIqmV5FpOZWY4DIqmUSp7FZGaW44BIvNSGmVlfDoik6gsGmZn14YBIyj4PwsysDwdE4kFqM7O+HBBJpVSi211MZmY9HBCJB6nNzPpyQCQepDYz68sBkXixPjOzvhwQSaVcomtHEOGQMDMDB0SPakkAXrDPzCxxQCSVcvajcDeTmVlmQAEhaaKkUrr/DkknS6oWW9rwqpazFoQHqs3MMgNtQfwImCBpOvA94I+B64oqqhkqtS4mtyDMzICBB4Qi4hXgvwL/GBGnAocXV9bw6+1icgvCzAwGERCSjgU+DnwnbSsXU1Jz9HYxuQVhZgYDD4jPAOcDt0bESkkHAncVV9bwq5TcgjAzy6sM5KCI+CHwQ4A0WL0hIs4psrDhVqm1IDwGYWYGDHwW079L2kPSROBRYJWkvyy2tOFVrY1BeBaTmRkw8C6mWRGxBTgFuB2YSTaTadTwLCYzs74GGhDVdN7DKcDyiOgERtUnaa0F0ekxCDMzYOAB8c/AOmAi8CNJBwBbiiqqGWpjEF5qw8wsM9BB6iuBK3ObnpQ0v5iSmqM2i8ktCDOzzEAHqSdLukxSe7p9maw18XrPO1HSGklrJS1tsH+8pJvS/nsktdbt31/SS5I+N8DvZ5fVzoPwGISZWWagXUzXAi8CH023LcBXX+sJksrAVcBJwCxgsaRZdYedAWyKiIOAy4FL6vZfRjYoXriKZzGZmfUxoC4m4O0R8ZHc44slPfQ6z5kLrI2IxwEkLQMWAKtyxywALkr3bwG+IkkREZJOAZ4AXh5gjW9IbRaTz4MwM8sMtAXxqqTjag8kvQd49XWeMx14Kve4I21reExEdAGbgamSJgF/BVz8Wl9A0pm1bq/169cP6BvZmaqX+zYz62OgLYhPAl+TNDk93gQsKaYkIGtVXB4RL0na6UERcTVwNUBbW9sb+mQv91wwyF1MZmYw8FlMDwPvkrRHerxF0meAR17jaU8DM3KPW9K2Rsd0SKoAk4GNwNHAQkl/B+wJ7JC0NSK+MpB6d0XVS22YmfUxqCvKRcSWdEY1wLmvc/h9wMGSZkoaBywCltcds5zelshC4M7I/F5EtEZEK3AF8D+LDAfoHaTudgvCzAwYeBdTIzvv+yEbU5B0NrCCbGnwa9NKsF8E2iNiOXANcIOktcDzZCHSFFUPUpuZ9fFGAuJ1P0kj4jbgtrptF+bubwVOfZ3XuGgX6xsUXzDIzKyv1wwISS/SOAgE7FZIRU3ipTbMzPp6zYCIiN2Hq5Bmq/YsteGAMDODQQ5Sj2Y9LQh3MZmZAQ6IHj1nUruLycwMcED0kESlJLcgzMwSB0ROpSwPUpuZJQ6InGqp5OtBmJklDoicSllerM/MLHFA5FTKJS/WZ2aWOCByqiX5PAgzs8QBkVMplzyLycwscUDkVMryeRBmZokDIqdacgvCzKzGAZHjWUxmZr0cEDmVcsldTGZmiQMip+qlNszMejggctzFZGbWywGRUy2X6PSJcmZmgAOij2w1V7cgzMzAAdFH2Yv1mZn1cEDkVL3ct5lZDwdETqVcotsBYWYGOCD6yBbrcxeTmRk4IPrwNFczs14OiBxfD8LMrJcDIsfXgzAz6+WAyPH1IMzMejkgcnw9CDOzXg6IHF8PwsyslwMip1IWOwJ2uBVhZuaAyKuWsx+HF+wzM3NA9FEpCcDnQpiZ4YDoo5JaEA4IMzMHRB/VctaCcBeTmZkDoo9KyS0IM7MaB0ROpdaC8FRXMzMHRF6ti8nXhDAzKzggJJ0oaY2ktZKWNtg/XtJNaf89klrT9vdJul/SL9K/xxdZZ01vF5NbEGZmhQWEpDJwFXASMAtYLGlW3WFnAJsi4iDgcuCStH0D8IcRcQSwBLihqDrzegapPQZhZlZoC2IusDYiHo+I7cAyYEHdMQuA69P9W4ATJCkiHoyI36TtK4HdJI0vsFYg14LwLCYzs0IDYjrwVO5xR9rW8JiI6AI2A1PrjvkI8EBEbKv/ApLOlNQuqX39+vVvuOCKWxBmZj1G9CC1pMPJup3+rNH+iLg6Itoiom3atGlv+OtVyx6DMDOrKTIgngZm5B63pG0Nj5FUASYDG9PjFuBW4BMR8asC6+xRLnkWk5lZTZEBcR9wsKSZksYBi4DldccsJxuEBlgI3BkRIWlP4DvA0oj4SYE19lH1eRBmZj0KC4g0pnA2sAJYDdwcESslfVHSyemwa4CpktYC5wK1qbBnAwcBF0p6KN32KarWmtogdbdbEGZmVIp88Yi4DbitbtuFuftbgVMbPO9LwJeKrK0RD1KbmfUa0YPUw61nkNrTXM3MHBB5vh6EmVkvB0ROzxXlPEhtZuaAyKt4sT4zsx4OiBwv1mdm1ssBkePF+szMejkgciqexWRm1sMBkVObxeQWhJmZA6KP3sX6HBBmZg6InHJJSO5iMjMDB0Q/1VLJXUxmZjgg+qmU5WmuZmY4IPqplOQT5czMcED0Uy2XvNSGmRkOiH6yLia3IMzMHBB1KqUSnZ7FZGbmgKhXdQvCzAxwQPRTKZd8HoSZGQ6Ifiol+TwIMzMcEP1UyyWfB2FmhgOin7LPgzAzAxwQ/VTL8nkQZmY4IPqplEp0uwVhZuaAqFcpe5DazAwcEP1UPc3VzAxwQPRTKflEOTMzcED048X6zMwyDog6lbKnuZqZgQOin0qp5C4mMzMcEP34PAgzs4wDoo67mMzMMg6IOpWSB6nNzMAB0Y+vB2FmlnFA1PH1IMzMMg6IOtV0PYgItyLMbGxzQNSplLMfiRfsM7OxzgFRp1IWgGcymdmY54CoUy1lPxLPZDKzsa7QgJB0oqQ1ktZKWtpg/3hJN6X990hqze07P21fI+kPCityxw7Y/krPw54WhGcymdkYVynqhSWVgauA9wEdwH2SlkfEqtxhZwCbIuIgSYuAS4CPSZoFLAIOB/YD7pD0jojoHvJC1z8G/3Qc7HMYTD+KWdtaeZcqnHfNNg6c0cJhB+zHO/bdg70mVtlzt3FMqJaQNORlmJmNNIUFBDAXWBsRjwNIWgYsAPIBsQC4KN2/BfiKsk/fBcCyiNgGPCFpbXq9nw15leN3h987F56+H1Yt5+itL/Af44Hns1vXQyVeYjc6KfMCFbqoECoRCCSgRE9bQyDI9pmZDZPfTjuOY876pyF/3SIDYjrwVO5xB3D0zo6JiC5Jm4GpafvP6547vf4LSDoTOBNg//3337Uq95wBx38+ux8Bzz8Oz62GVzfR/crzvLDhWV568QU6t2+nq3M73Z3b2LGjm9ixg4ggIhuriNrz+3FXlZkVbI/9CnnZIgOicBFxNXA1QFtb2xv/JJZg6tuzG1AG9k43M7OxpshB6qeBGbnHLWlbw2MkVYDJwMYBPtfMzApUZEDcBxwsaaakcWSDzsvrjlkOLEn3FwJ3RnYK83JgUZrlNBM4GLi3wFrNzKxOYV1MaUzhbGAFWW/NtRGxUtIXgfaIWA5cA9yQBqGfJwsR0nE3kw1odwGfKmQGk5mZ7ZRGy5pDbW1t0d7e3uwyzMzeVCTdHxFtjfb5TGozM2vIAWFmZg05IMzMrCEHhJmZNTRqBqklrQeefAMvsTewYYjKGUqua3Bc1+C4rsEZjXUdEBHTGu0YNQHxRklq39lIfjO5rsFxXYPjugZnrNXlLiYzM2vIAWFmZg05IHpd3ewCdsJ1DY7rGhzXNThjqi6PQZiZWUNuQZiZWUMOCDMza2jMB4SkEyWtkbRW0tIm1nGtpOckPZrbNkXS9yX9Mv27VxPqmiHpLkmrJK2U9OmRUJukCZLulfRwquvitH2mpHvS7/OmtNT8sJNUlvSgpG+PlLokrZP0C0kPSWpP20bCe2xPSbdIekzSaknHNrsuSYekn1PttkXSZ5pdV6rts+k9/6ikG9P/hULeX2M6ICSVgauAk4BZwGJJs5pUznXAiXXblgI/iIiDgR+kx8OtC/iLiJgFHAN8Kv2Mml3bNuD4iHgXMBs4UdIxwCXA5RFxELAJOGOY66r5NLA693ik1DU/Imbn5sw3+/cI8PfAdyPiUOBdZD+3ptYVEWvSz2k28DvAK8Ctza5L0nTgHKAtIt5JdimFRRT1/squqzw2b8CxwIrc4/OB85tYTyvwaO7xGuBt6f7bgDUj4Gf2H8D7RlJtwFuAB8iueb4BqDT6/Q5jPS1kHx7HA98GNELqWgfsXbetqb9HsqtIPkGaMDNS6qqr5f3AT0ZCXcB04ClgCtn1fL4N/EFR768x3YKg94dd05G2jRRvjYhn0v3fAm9tZjGSWoE5wD2MgNpSN85DwHPA94FfAS9ERFc6pFm/zyuA84Ad6fHUEVJXAN+TdL+kM9O2Zv8eZwLrga+mLrl/lTRxBNSVtwi4Md1val0R8TRwKfBr4BlgM3A/Bb2/xnpAvGlE9qdB0+YkS5oEfAP4TERsye9rVm0R0R1ZF0ALMBc4dLhrqCfpQ8BzEXF/s2tp4LiIOIqsS/VTkn4/v7NJv8cKcBTwvyNiDvAydd02zXzvp778k4H/U7+vGXWlMY8FZMG6HzCR/l3TQ2asB8TTwIzc45a0baR4VtLbANK/zzWjCElVsnD4t4j45kiqDSAiXgDuImta7ympdindZvw+3wOcLGkdsIysm+nvR0Bdtb8+iYjnyPrT59L832MH0BER96THt5AFRrPrqjkJeCAink2Pm13XfwGeiIj1EdEJfJPsPVfI+2usB8R9wMFpBsA4sqbk8ibXlLccWJLuLyHr/x9WkkR27fDVEXHZSKlN0jRJe6b7u5GNi6wmC4qFzaorIs6PiJaIaCV7P90ZER9vdl2SJkravXafrF/9UZr8e4yI3wJPSTokbTqB7Fr0TX/vJ4vp7V6C5tf1a+AYSW9J/zdrP69i3l/NGvgZKTfgA8D/I+u//h9NrONGsj7FTrK/qs4g67v+AfBL4A5gShPqOo6sGf0I8FC6faDZtQFHAg+muh4FLkzbDwTuBdaSdQuMb+LvdB7w7ZFQV/r6D6fbytp7vdm/x1TDbKA9/S6/Bew1QuqaCGwEJue2jYS6LgYeS+/7G4DxRb2/vNSGmZk1NNa7mMzMbCccEGZm1pADwszMGnJAmJlZQw4IMzNryAFhNgiSuutW+RyyxdoktSq3mq9Zs1Ve/xAzy3k1suU9zEY9tyDMhkC61sLfpest3CvpoLS9VdKdkh6R9ANJ+6ftb5V0a7qexcOS3p1eqizpX9J6/99LZ4mbNYUDwmxwdqvrYvpYbt/miDgC+ArZiq4A/wBcHxFHAv8GXJm2Xwn8MLLrWRxFdnYzwMHAVRFxOPAC8JGCvx+znfKZ1GaDIOmliJjUYPs6sgsYPZ4WN/xtREyVtIHs+gGdafszEbG3pPVAS0Rsy71GK/D9yC5Gg6S/AqoR8aXivzOz/tyCMBs6sZP7g7Etd78bjxNaEzkgzIbOx3L//izd/ynZqq4AHwd+nO7/ADgLei58NHm4ijQbKP91YjY4u6Wr2NV8NyJqU133kvQIWStgcdr252RXS/tLsiun/Una/mngaklnkLUUziJbzddsxPAYhNkQSGMQbRGxodm1mA0VdzGZmVlDbkGYmVlDbkGYmVlDDggzM2vIAWFmZg05IMzMrCEHhJmZNfT/AY/MtIfAXhtIAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RMSE is\n",
            "0.0\n",
            "Validation RMSE is\n",
            "0.0\n",
            "Training loss is\n",
            "0.0\n",
            "Validation loss is\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Insert PubChem Fingerprint of Drug and Carrier (Oil, Surfactant, Cosurfactant)\n",
        "!pip install pubchempy\n",
        "import tensorflow as tf\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import io\n",
        "from tensorflow import keras\n",
        "import pubchempy as pcp\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.models import load_model\n",
        "import time as tm\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "#gdd.download_file_from_google_drive(file_id='1jWa1UK8cxcCuEX_Chs1zm4hfa_j_Q-Ki',\n",
        "                                   #dest_path='/content/sample_data/HPLC_NUM.hdf5',\n",
        "                                   #unzip=False)\n",
        "#Load Model\n",
        "model = load_model('/content/sample_data/HPLC_NUM.hdf5', compile= True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-1CGTx7sY7z",
        "outputId": "69a04502-ca78-4925-df94-d683921cab48"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pubchempy in /usr/local/lib/python3.8/dist-packages (1.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Drug_1_CID =  2244#@param {type:\"number\"}\n",
        "Drug_2_CID =  24872560#@param {type:\"number\"}\n",
        "Methanol =  0#@param {type:\"number\"}\n",
        "Methanol = Methanol/100\n",
        "Acetonitrile =  40#@param {type:\"number\"}\n",
        "Acetonitrile = Acetonitrile/100\n",
        "Water =  0#@param {type:\"number\"}\n",
        "Water = Water/100\n",
        "Buffer =  60#@param {type:\"number\"}\n",
        "Buffer = Buffer/100\n",
        "pH =  4#@param {type:\"number\"}\n",
        "pH = pH/14\n",
        "\n",
        "\n",
        "Drug_1_CID = pcp.Compound.from_cid(Drug_1_CID)\n",
        "Drug_2_CID = pcp.Compound.from_cid(Drug_2_CID)\n",
        "\n",
        "FPD_1 = Drug_1_CID.cactvs_fingerprint\n",
        "FPD_2 = Drug_2_CID.cactvs_fingerprint\n",
        "\n",
        "List1 = [Methanol, Acetonitrile, Water, Buffer, pH]\n",
        "\n",
        "List2 = list(FPD_1)\n",
        "List3 = list(FPD_2)\n",
        "List = List1+List2 + List3\n",
        "t = pd.DataFrame(np.array(List).reshape(-1,len(List)))\n",
        "\n",
        "dataset1 = t.values\n",
        "t.dtype = int\n",
        "X_Predict = (dataset1[:,0:1767].astype(float))\n",
        "\n",
        "\n",
        "Y_prediction = model.predict(X_Predict)\n",
        "Y_prediction = Y_prediction*50\n",
        "Y_prediction = pd.DataFrame(Y_prediction)\n",
        "print(Y_prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spuSzwGes2rj",
        "outputId": "89a27b1e-8225-4485-ebcc-bf3aa606de1e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 67ms/step\n",
            "     0    1\n",
            "0  0.0  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YsJ8il8A2yxW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuViSjwJX+a4cD/8du0GGk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}